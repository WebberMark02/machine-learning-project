{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WebberMark02/machine-learning-project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importo le librerie necessarie e scelgo di utilizzare \"tensorflow\" come\n",
        "backend per \"Keras\"."
      ],
      "metadata": {
        "id": "gNcUV4pt1_KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist"
      ],
      "metadata": {
        "id": "VAXrS08i2CjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imposto le variabili globali."
      ],
      "metadata": {
        "id": "Seh4hYeV2Osd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 142\n",
        "training_set_size = 1000000\n",
        "testing_set_size = 5000\n",
        "validation_set_size = 20000\n",
        "image_shape = (32, 32, 1)\n",
        "batch_size = 10000\n",
        "epochs = 50\n",
        "learning_rate = 0.005\n",
        "early_stopping_patience = 50\n",
        "reduce_lr_patience = 5\n",
        "l2 = 0.0\n",
        "training_backup_path = './daniele-russo-0001028215-ml-february-2025-trainingbackup/'\n",
        "net_parameters_path = './daniele-russo-0001028215-ml-february-2025-network.weights.h5'"
      ],
      "metadata": {
        "id": "C8nN3iR52JSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco il generatore di immagini sulle quali il modello\n",
        "verrà addestrato e testato.  \n",
        "Il generatore restituisce \"batchsize\" immagini; ogni immagine è\n",
        "la media di due immagini scelte casualmente rispettivamente da 'x1' e 'x2'.  \n",
        "Il generatore restituisce, inoltre, per ogni media di immagini, la coppia delle immagini delle quali è stata calcolata la media stessa."
      ],
      "metadata": {
        "id": "fTrSU0rn2Y3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def datagenerator(x1, x2, batchsize):\n",
        "    n1 = x1.shape[0]\n",
        "    n2 = x2.shape[0]\n",
        "    while True:\n",
        "        num1 = np.random.randint(0, n1, batchsize)\n",
        "        num2 = np.random.randint(0, n2, batchsize)\n",
        "\n",
        "        x_data = (x1[num1] + x2[num2]) / 2.0\n",
        "        y_data = (x1[num1], x2[num2])\n",
        "\n",
        "        yield x_data, y_data"
      ],
      "metadata": {
        "id": "Msxd-j1j2g5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco una funzione per il controllo del bilanciamento delle classi.  \n",
        "Mi servirà per verificare che la divisione stratificata abbia avuto successo."
      ],
      "metadata": {
        "id": "0DKB7LG33wRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stampa_percentuale(y, title = \"\"):\n",
        "  if title:\n",
        "    print(title)\n",
        "  # Calcolo le occorrenze di ciascuna classe nel dataset.\n",
        "  unique, counts = np.unique(y, return_counts = True)\n",
        "  # Calcolo la % di occorrenze per ciascuna classe.\n",
        "  percentuali = (counts / len(y)) * 100\n",
        "  # Stampo le occorrenze e le percentuali.\n",
        "  for classe, conteggio, percentuale in zip(unique, counts, percentuali):\n",
        "      print(f\"Classe {classe}: Occorrenze = {conteggio}, Percentuale {percentuale} %\" )\n",
        "  print(f\"Totale occorrenze : {sum(counts)}\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "3z92DZs935J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco un modello banale.\n",
        "Mi servirà per valutare che la rete abbia prestazioni migliori di esso."
      ],
      "metadata": {
        "id": "mdVdsGyY38FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ide_model(x):\n",
        "   return((x,x))"
      ],
      "metadata": {
        "id": "Vyn_ESXT4F3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora ha inizio la fase di caricamento e preparazione dei dataset che verranno utilizzati\n",
        "per addestrare e esaminare le prestazioni della rete.\n",
        "\n",
        "Prima di tutto, carico i training set e i testing set di \"MNIST\" e \"Fashion MNIST\"."
      ],
      "metadata": {
        "id": "mKhtLevU4QS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
        "(fashion_mnist_x_train, fashion_mnist_y_train), (fashion_mnist_x_test, fashion_mnist_y_test) = fashion_mnist.load_data()\n",
        "\n",
        "print(np.shape(mnist_x_train))"
      ],
      "metadata": {
        "id": "naZI3jI34Qgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridimensiono le immagini tramite padding, portando la loro risoluzione da 28x28 a 32x32.  \n",
        "Inoltre, le normalizzo nell'intervallo [0, 1]."
      ],
      "metadata": {
        "id": "SgRovOk-4n4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize in and pad\n",
        "mnist_x_train = np.pad(mnist_x_train,((0,0),(2,2),(2,2)))/255.\n",
        "mnist_x_test = np.pad(mnist_x_test,((0,0),(2,2),(2,2)))/255.\n",
        "fashion_mnist_x_train = np.pad(fashion_mnist_x_train,((0,0),(2,2),(2,2)))/255.\n",
        "fashion_mnist_x_test = np.pad(fashion_mnist_x_test,((0,0),(2,2),(2,2)))/255.\n",
        "\n",
        "print(np.shape(mnist_x_train))"
      ],
      "metadata": {
        "id": "6EkD9Wb246KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aggiungo una dimensione agli array numpy delle immagini (non modifico affatto\n",
        "le immagini).  \n",
        "Mi serve per rendere le immagini compatibili con le dimensioni\n",
        "del layer di input della rete neurale."
      ],
      "metadata": {
        "id": "xnYC3egjSTaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.shape(mnist_x_train))\n",
        "print(np.shape(mnist_x_test))\n",
        "print(np.shape(fashion_mnist_x_train))\n",
        "print(np.shape(fashion_mnist_x_test))\n",
        "\n",
        "mnist_x_train = np.reshape(mnist_x_train, (mnist_x_train.shape[0], mnist_x_train.shape[1], mnist_x_train.shape[2], 1))\n",
        "mnist_x_test = np.reshape(mnist_x_test, (mnist_x_test.shape[0], mnist_x_test.shape[1], mnist_x_test.shape[2], 1))\n",
        "fashion_mnist_x_train = np.reshape(fashion_mnist_x_train, (fashion_mnist_x_train.shape[0], fashion_mnist_x_train.shape[1], fashion_mnist_x_train.shape[2], 1))\n",
        "fashion_mnist_x_test = np.reshape(fashion_mnist_x_test, (fashion_mnist_x_test.shape[0], fashion_mnist_x_test.shape[1], fashion_mnist_x_test.shape[2], 1))\n",
        "\n",
        "print(np.shape(mnist_x_train))\n",
        "print(np.shape(mnist_x_test))\n",
        "print(np.shape(fashion_mnist_x_train))\n",
        "print(np.shape(fashion_mnist_x_test))"
      ],
      "metadata": {
        "id": "IuJwFof8Sj0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizzo qualche immagine per accertarmi che l'operazione di reshaping non le abbia modificate."
      ],
      "metadata": {
        "id": "KthA4eTETbKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(mnist_x_train[0], cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(mnist_x_test[0], cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(fashion_mnist_x_train[0], cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(fashion_mnist_x_test[0], cmap='gray')\n",
        "plt.show()\n",
        "\n",
        "print(mnist_x_train[0].shape)"
      ],
      "metadata": {
        "id": "ji4W6Yw-Tg-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Controllo il bilanciamento delle classi nei training set prima della divisione stratificata."
      ],
      "metadata": {
        "id": "dn2OwqKL7rbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stampa_percentuale(mnist_y_train, 'MNIST training set completo')\n",
        "stampa_percentuale(fashion_mnist_y_train, 'Fashion MNIST training set completo')"
      ],
      "metadata": {
        "id": "a7Xsfvub7uS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divido ogni training set in due insiemi: il training set e il validation set.\n",
        "Le immagini dei validation set verranno usate per l'ottimizzazione degli iper-parametri della rete.\n",
        "Ogni validation set conterrà il 20% delle immagini del training set di partenza.\n",
        "Uso la stratificazione per mantenere le classi nelle stesse proporzioni."
      ],
      "metadata": {
        "id": "b29NjrbE-V-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_x_train, mnist_x_val, mnist_y_train, mnist_y_val = sklearn.model_selection.train_test_split(mnist_x_train, mnist_y_train, test_size=0.2, stratify=mnist_y_train, random_state=seed)\n",
        "fashion_mnist_x_train, fashion_mnist_x_val, fashion_mnist_y_train, fashion_mnist_y_val = sklearn.model_selection.train_test_split(fashion_mnist_x_train, fashion_mnist_y_train, test_size=0.2, stratify=fashion_mnist_y_train, random_state=seed)"
      ],
      "metadata": {
        "id": "xENN8DNY-uZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Controllo il bilanciamento delle classi nei training set e nei validation set ottenuti dalla divisione stratificata."
      ],
      "metadata": {
        "id": "s8vTNUf2_VSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stampa_percentuale(mnist_y_train, 'MNIST training set risultante')\n",
        "stampa_percentuale(mnist_y_val, 'MNIST validation set risultante')\n",
        "\n",
        "stampa_percentuale(fashion_mnist_y_train, 'Fashion MNIST training set risultante')\n",
        "stampa_percentuale(fashion_mnist_y_val, 'Fashion MNIST validation set risultante')"
      ],
      "metadata": {
        "id": "Y88auHto_ccW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creo tre istanze del generatore per generare il training set, il validation set e il testing set finali."
      ],
      "metadata": {
        "id": "k4nmLpj8AjVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "traingen = datagenerator(mnist_x_train, fashion_mnist_x_train, training_set_size)\n",
        "valgen = datagenerator(mnist_x_val, fashion_mnist_x_val, validation_set_size)\n",
        "testgen = datagenerator(mnist_x_test, fashion_mnist_x_test, testing_set_size)"
      ],
      "metadata": {
        "id": "Isn3rYuF_reG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creo il training set e il validation set."
      ],
      "metadata": {
        "id": "LDBwhTE0BUC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = next(traingen)\n",
        "x_val, y_val = next(valgen)"
      ],
      "metadata": {
        "id": "TiPGk0YQBN9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifico che le dimensioni dei due dataset siano corrette."
      ],
      "metadata": {
        "id": "4onlfynIBmUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(x_val.shape)\n",
        "\n",
        "print(len(y_train))\n",
        "print(len(y_val))\n",
        "\n",
        "print(y_train[0].shape)\n",
        "print(y_train[1].shape)\n",
        "print(y_val[0].shape)\n",
        "print(y_val[1].shape)"
      ],
      "metadata": {
        "id": "7XXFelsHBZAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Controllo che i valori dei pixel delle immagini appartengano all'intervallo [0,1]."
      ],
      "metadata": {
        "id": "aXH31VvmB4Ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.min(x_train[0]), np.max(x_train[0]))\n",
        "print(np.min(x_val[0]), np.max(x_val[0]))"
      ],
      "metadata": {
        "id": "aJZI6GD3Bq_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostro qualche immagine dei due dataset."
      ],
      "metadata": {
        "id": "K_xSO2LgUBP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(x_train[0], cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(y_train[0][0], cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(y_train[1][0], cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(x_val[0], cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(y_val[0][0], cmap='gray')\n",
        "plt.show()\n",
        "plt.imshow(y_val[1][0], cmap='gray')\n",
        "\n",
        "print(x_train[0].shape)"
      ],
      "metadata": {
        "id": "fMb4caLeUDy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fase di preparazione dei dataset è completa.\n",
        "Ora viene definita e compilata la rete neurale.  \n",
        "    \n",
        "Essa è formata da due autoencoder che condividono lo stesso layer di input.  \n",
        "Ogni autoencoder restituisce una delle due immagini la cui media è l'immagine di partenza, ricostruendola il più fedelmente possibile.  \n",
        "  \n",
        "In uno dei primi addestramenti effettuati, ho notato che la rete ricostruiva con altissima fedeltà l'immagine di \"Fashion MNIST\", ma non ricostruiva affatto l'immagine di \"MNIST\".  \n",
        "Di conseguenza, per obbligare la rete a dare maggiore priorità\n",
        "alla ricostruzione della immagine di \"MNIST\", ho applicato la regolarizzazione L2 sui layer convoluzionali dell'autoencoder che ricostruisce l'immagine di \"Fashion MNIST\".  \n",
        "  \n",
        "Nei layer convoluzionali, ho impostato il parametro \"padding\" a \"same\" per garantire che le dimensioni dei due layer di output siano uguali a quelle del layer di input."
      ],
      "metadata": {
        "id": "EMfzMLnICB5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_neural_network():\n",
        "    # Input Layer\n",
        "    inputs = keras.Input(shape = image_shape, name = 'InputImage')\n",
        "\n",
        "    # Encoder 1\n",
        "    enc1_conv1 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', name = 'Enc1_Conv1', kernel_initializer='random_normal', bias_initializer='zeros')(inputs)\n",
        "    enc1_pool1 = keras.layers.MaxPooling2D((2, 2), padding='same', name = 'Enc1_Pool1')(enc1_conv1)\n",
        "    enc1_conv2 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name = 'Enc1_Conv2', kernel_initializer='random_normal', bias_initializer='zeros')(enc1_pool1)\n",
        "    enc1_pool2 = keras.layers.MaxPooling2D((2, 2), padding='same', name = 'Enc1_Pool2')(enc1_conv2)\n",
        "    enc1_conv3 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', name = 'Enc1_Conv3', kernel_initializer='random_normal', bias_initializer='zeros')(enc1_pool2)\n",
        "    encoded1 = keras.layers.MaxPooling2D((2, 2), padding='same', name = 'Encoded1')(enc1_conv3)\n",
        "\n",
        "    # Encoder 2\n",
        "    enc2_conv1 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', name = 'Enc2_Conv1', kernel_initializer='random_normal', bias_initializer='zeros', kernel_regularizer=keras.regularizers.L2(l2), bias_regularizer=keras.regularizers.L2(l2))(inputs)\n",
        "    enc2_pool1 = keras.layers.MaxPooling2D((2, 2), padding='same', name = 'Enc2_Pool1')(enc2_conv1)\n",
        "    enc2_conv2 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name = 'Enc2_Conv2', kernel_initializer='random_normal', bias_initializer='zeros', kernel_regularizer=keras.regularizers.L2(l2), bias_regularizer=keras.regularizers.L2(l2))(enc2_pool1)\n",
        "    enc2_pool2 = keras.layers.MaxPooling2D((2, 2), padding='same', name = 'Enc2_Pool2')(enc2_conv2)\n",
        "    enc2_conv3 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', name = 'Enc2_Conv3', kernel_initializer='random_normal', bias_initializer='zeros', kernel_regularizer=keras.regularizers.L2(l2), bias_regularizer=keras.regularizers.L2(l2))(enc2_pool2)\n",
        "    encoded2 = keras.layers.MaxPooling2D((2, 2), padding='same', name = 'Encoded2')(enc2_conv3)\n",
        "\n",
        "    # Decoder 1\n",
        "    dec1_conv1 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', name = 'Dec1_Conv1', kernel_initializer='random_normal', bias_initializer='zeros')(encoded1)\n",
        "    dec1_upsampling1 = keras.layers.UpSampling2D((2, 2), name = 'Dec1_Upsampling1')(dec1_conv1)\n",
        "    dec1_conv2 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name = 'Dec1_Conv2', kernel_initializer='random_normal', bias_initializer='zeros')(dec1_upsampling1)\n",
        "    dec1_upsampling2 = keras.layers.UpSampling2D((2, 2), name = 'Dec1_Upsampling2')(dec1_conv2)\n",
        "    dec1_conv3 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', name = 'Dec1_Conv3', kernel_initializer='random_normal', bias_initializer='zeros')(dec1_upsampling2)\n",
        "    dec1_upsampling3 = keras.layers.UpSampling2D((2, 2), name = 'Dec1_Upsampling3')(dec1_conv3)\n",
        "    decoded1 = keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same', name = 'MNIST_Image', kernel_initializer='random_normal', bias_initializer='zeros')(dec1_upsampling3)\n",
        "\n",
        "    # Decoder 2\n",
        "    dec2_conv1 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same', name = 'Dec2_Conv1', kernel_initializer='random_normal', bias_initializer='zeros', kernel_regularizer=keras.regularizers.L2(l2), bias_regularizer=keras.regularizers.L2(l2))(encoded2)\n",
        "    dec2_upsampling1 = keras.layers.UpSampling2D((2, 2), name = 'Dec2_Upsampling1')(dec2_conv1)\n",
        "    dec2_conv2 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name = 'Dec2_Conv2', kernel_initializer='random_normal', bias_initializer='zeros', kernel_regularizer=keras.regularizers.L2(l2), bias_regularizer=keras.regularizers.L2(l2))(dec2_upsampling1)\n",
        "    dec2_upsampling2 = keras.layers.UpSampling2D((2, 2), name = 'Dec2_Upsampling2')(dec2_conv2)\n",
        "    dec2_conv3 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', name = 'Dec2_Conv3', kernel_initializer='random_normal', bias_initializer='zeros', kernel_regularizer=keras.regularizers.L2(l2), bias_regularizer=keras.regularizers.L2(l2))(dec2_upsampling2)\n",
        "    dec2_upsampling3 = keras.layers.UpSampling2D((2, 2), name = 'Dec2_Upsampling3')(dec2_conv3)\n",
        "    decoded2 = keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same', name = 'Fashion_MNIST_Image', kernel_initializer='random_normal', bias_initializer='zeros', kernel_regularizer=keras.regularizers.L2(l2), bias_regularizer=keras.regularizers.L2(l2))(dec2_upsampling3)\n",
        "\n",
        "    model = keras.Model(inputs = inputs, outputs = [decoded1, decoded2], name = 'MNIST_Reconstruction_Model')\n",
        "    return model"
      ],
      "metadata": {
        "id": "25JonRhnB620"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Istanzio la rete e mostro i suoi dettagli."
      ],
      "metadata": {
        "id": "hHLaHd0nKYy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_neural_network()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "BcbYrLF7KXtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizzo un plot della rete."
      ],
      "metadata": {
        "id": "duLY3VwqMkv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(model, \"model.png\", show_shapes = True, show_layer_names = True)"
      ],
      "metadata": {
        "id": "1ATMl5ntKcye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco la funzione di costo che l'addestramento minimizzerà il più possibile."
      ],
      "metadata": {
        "id": "ZhSyZ298i5cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse_loss = keras.losses.MeanSquaredError(\n",
        "    reduction = \"sum_over_batch_size\",\n",
        "    name = \"mean_squared_error\"\n",
        ")"
      ],
      "metadata": {
        "id": "GZVnddnEjFAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilo la rete."
      ],
      "metadata": {
        "id": "R_PT0ZSwN9iF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = [mse_loss, mse_loss],\n",
        "    optimizer = keras.optimizers.AdamW(learning_rate = learning_rate),\n",
        ")"
      ],
      "metadata": {
        "id": "XF25EscDMopF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco una callback EarlyStopping. Essa valuta alla fine di ogni epoca la funzione di costo sul validation set e decide se fermare l'addestramento oppure no. Utile per stabilire automaticamente un buon numero di epoche di addestramento della rete."
      ],
      "metadata": {
        "id": "YK3VpLnZOzQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor = 'val_loss',   # Monitora la loss sulla metrica indicata\n",
        "    min_delta = 0.001,      # Variazione minima da considerare come miglioramento\n",
        "    patience = early_stopping_patience,          # Numero di epoche senza miglioramenti prima di fermare l'addestramento\n",
        "    mode = 'auto',          # oppure prende \"min\", \"max\", seleziona la direzione in automatico\n",
        "    restore_best_weights = True,  # Ripristina i pesi migliori quando l'addestramento si ferma\n",
        "    start_from_epoch = 5    # Inizia il monitoraggio dall'epoca 5\n",
        ")"
      ],
      "metadata": {
        "id": "EKXKUyu3OtBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco una callback \"BackupAndRestore\".  \n",
        "Alla fine di ogni epoca di addestramento, essa salva la rete in un file di backup temporaneo.  \n",
        "Se il notebook dovesse bloccarsi a tempo di esecuzione, sarà possibile riavviare l'addestramento ripristinando l'ultimo stato salvato nel file di backup."
      ],
      "metadata": {
        "id": "Wq84_EZnk8w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backup_and_restore = keras.callbacks.BackupAndRestore(backup_dir = training_backup_path)"
      ],
      "metadata": {
        "id": "d1kLCloDlQk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco una callback per il controllo del learning rate."
      ],
      "metadata": {
        "id": "fhs6nPEqlYRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor = 'val_loss',\n",
        "    min_delta = 0.001,\n",
        "    factor = 0.2,\n",
        "    patience = reduce_lr_patience,\n",
        "    min_lr = 0.001\n",
        ")"
      ],
      "metadata": {
        "id": "JfaFCAsmlbAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco due funzioni.  \n",
        "Una che congela il primo autoencoder.  \n",
        "Una che congela il secondo autoencoder.  "
      ],
      "metadata": {
        "id": "g3upzuBwcoD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def freeze_unfreeze_autoencoder(model, isTrainable, autoencoder):\n",
        "  output_layer_name = 'MNIST' if autoencoder == 1 else 'Fashion_MNIST'\n",
        "\n",
        "  model.get_layer(f'Enc{autoencoder}_Conv1').trainable = isTrainable\n",
        "  model.get_layer(f'Enc{autoencoder}_Pool1').trainable = isTrainable\n",
        "  model.get_layer(f'Enc{autoencoder}_Conv2').trainable = isTrainable\n",
        "  model.get_layer(f'Enc{autoencoder}_Pool2').trainable = isTrainable\n",
        "  model.get_layer(f'Enc{autoencoder}_Conv3').trainable = isTrainable\n",
        "  model.get_layer(f'Encoded{autoencoder}').trainable = isTrainable\n",
        "  model.get_layer(f'Dec{autoencoder}_Conv1').trainable = isTrainable\n",
        "  model.get_layer(f'Dec{autoencoder}_Upsampling1').trainable = isTrainable\n",
        "  model.get_layer(f'Dec{autoencoder}_Conv2').trainable = isTrainable\n",
        "  model.get_layer(f'Dec{autoencoder}_Upsampling2').trainable = isTrainable\n",
        "  model.get_layer(f'Dec{autoencoder}_Conv3').trainable = isTrainable\n",
        "  model.get_layer(f'Dec{autoencoder}_Upsampling3').trainable = isTrainable\n",
        "  model.get_layer(f'{output_layer_name}_Image').trainable = isTrainable"
      ],
      "metadata": {
        "id": "GDn1wyGNcxS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se è disponibile un file di pesi del modello, carico i pesi e salto\n",
        "l'addestramento.  \n",
        "Altrimenti, addestro il modello e salvo i pesi risultanti."
      ],
      "metadata": {
        "id": "RyIa-tbIPGlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded = True\n",
        "try:\n",
        "  model.load_weights(net_parameters_path)\n",
        "  print('Pesi caricati')\n",
        "except:\n",
        "  loaded = False\n",
        "  print('Nessun file dei pesi trovato')\n",
        "  history = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = (x_val, y_val), callbacks = [early_stopping, backup_and_restore, reduce_lr])\n",
        "  model.save_weights(net_parameters_path)\n",
        "  print('Pesi salvati')"
      ],
      "metadata": {
        "id": "ab9vDfmTPJjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora definisco una funzione per la creazione di grafici della storia dell'addestramento e se non è stato eseguito l'addestramento, allora visualizzo quattro grafici che la mostrano."
      ],
      "metadata": {
        "id": "xYD1-Rj5A903"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_training_history(history):\n",
        "\n",
        "    training_loss = history.history['loss']\n",
        "    validation_loss = history.history['val_loss']\n",
        "    mnist_training_loss = history.history['MNIST_Image_loss']\n",
        "    mnist_validation_loss = history.history['val_MNIST_Image_loss']\n",
        "    fashion_mnist_training_loss = history.history['Fashion_MNIST_Image_loss']\n",
        "    fashion_mnist_validation_loss = history.history['val_Fashion_MNIST_Image_loss']\n",
        "    learning_rate = history.history['learning_rate']\n",
        "\n",
        "    # Crea un grafico\n",
        "    epochs = range(1, len(training_loss) + 1)\n",
        "    plt.figure(figsize=(12, 12))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(epochs, training_loss, label='Training Loss')\n",
        "    plt.plot(epochs, validation_loss, label='Validation Loss')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(epochs, mnist_training_loss, label='MNIST Training Loss')\n",
        "    plt.plot(epochs, mnist_validation_loss, label='MNIST Validation Loss')\n",
        "    plt.title('MNIST Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.plot(epochs, fashion_mnist_training_loss, label='Fashion MNIST Training Loss')\n",
        "    plt.plot(epochs, fashion_mnist_validation_loss, label='Fashion MNIST Validation Loss')\n",
        "    plt.title('Fashion MNIST Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.plot(epochs, learning_rate)\n",
        "    plt.title('Learning Rate')\n",
        "    plt.xlabel('Epochs')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "if not loaded:\n",
        "  plot_training_history(history)"
      ],
      "metadata": {
        "id": "boJA_MAKA-hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco due funzioni per la valutazione finale del modello."
      ],
      "metadata": {
        "id": "r6x9HCn8BjIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_model(model):\n",
        "  x, (y1, y2) = next(testgen)\n",
        "  if isinstance(model, keras.Model):\n",
        "    pred1, pred2 = model.predict(x)\n",
        "  else:\n",
        "    pred1, pred2 = model(x)\n",
        "\n",
        "  return (np.mean((pred1-y1)**2) + np.mean((pred2-y2)**2)) / 2\n",
        "\n",
        "def multiple_eval_model(model, repeat_eval = 10):\n",
        "  eval_results = []\n",
        "  for i in range(repeat_eval):\n",
        "    eval_results.append(eval_model(model))\n",
        "  print(\"mse mean = \", np.mean(eval_results))\n",
        "  print(\"mse standard deviation = \", np.std(eval_results))"
      ],
      "metadata": {
        "id": "x_TV0-rpBljX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Valuto la rete e il modello casuale e confronto le loro prestazioni.  \n",
        "Più questi valori sono vicini a zero, più la rete è accurata."
      ],
      "metadata": {
        "id": "KV2WbHMVCcoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Valutazione modello banale')\n",
        "multiple_eval_model(ide_model)\n",
        "\n",
        "print('Valutazione rete neurale')\n",
        "multiple_eval_model(model)"
      ],
      "metadata": {
        "id": "pvHxVF3qCf1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mostro 30 esempi di previsioni."
      ],
      "metadata": {
        "id": "8d4aMGW0Pjm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onegen = datagenerator(mnist_x_test, fashion_mnist_x_test, 1)\n",
        "\n",
        "def show_images(x, y1, y2):\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(12,4))\n",
        "    ax[0].imshow(x,cmap='gray')\n",
        "    ax[0].title.set_text('Input')\n",
        "    ax[0].axis('off')\n",
        "    ax[1].imshow(y1,cmap='gray')\n",
        "    ax[1].title.set_text('mnist')\n",
        "    ax[1].axis('off')\n",
        "    ax[2].imshow(y2,cmap='gray')\n",
        "    ax[2].title.set_text('fashion_mnist')\n",
        "    ax[2].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "for i in range(1, 30):\n",
        "  print('Campione di esempio numero ', i)\n",
        "  x, (y1, y2) = next(onegen)\n",
        "  show_images(x[0], y1[0], y2[0])\n",
        "\n",
        "  print('Previsione del modello banale')\n",
        "  y1_pred, y2_pred = ide_model(x)\n",
        "  show_images(x[0], y1_pred[0], y2_pred[0])\n",
        "\n",
        "  print('Previsione della rete neurale')\n",
        "  y1_pred, y2_pred = model.predict(x)\n",
        "  show_images(x[0], y1_pred[0], y2_pred[0])\n",
        "\n",
        "  print()\n",
        "  print()"
      ],
      "metadata": {
        "id": "VYDq5CNzOFb7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}