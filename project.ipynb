{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WebberMark02/machine-learning-project/blob/main/project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importo le librerie necessarie e scelgo di utilizzare \"tensorflow\" come\n",
        "backend per \"Keras\"."
      ],
      "metadata": {
        "id": "gNcUV4pt1_KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "import sklearn\n",
        "from tensorflow.keras.datasets import mnist, fashion_mnist"
      ],
      "metadata": {
        "id": "VAXrS08i2CjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imposto le variabili globali."
      ],
      "metadata": {
        "id": "Seh4hYeV2Osd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "training_set_size = 180000\n",
        "testing_set_size = 5000\n",
        "validation_set_size = 20000\n",
        "image_shape = (32, 32, 1)\n",
        "batch_size = 256\n",
        "epochs = 250\n",
        "learning_rate = 0.01\n",
        "early_stopping_patience = 50\n",
        "reduce_lr_patience = 10"
      ],
      "metadata": {
        "id": "C8nN3iR52JSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco il generatore di immagini sulle quali il modello\n",
        "verrà addestrato e testato.  \n",
        "Il generatore restituisce \"batchsize\" immagini; ogni immagine è\n",
        "la media di due immagini scelte casualmente rispettivamente da 'x1' e 'x2'.  \n",
        "Il generatore restituisce, inoltre, per ogni media di immagini, la coppia delle immagini delle quali è stata calcolata la media stessa."
      ],
      "metadata": {
        "id": "fTrSU0rn2Y3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def datagenerator(x1, x2, batchsize):\n",
        "    n1 = x1.shape[0]\n",
        "    n2 = x2.shape[0]\n",
        "    while True:\n",
        "        num1 = np.random.randint(0, n1, batchsize)\n",
        "        num2 = np.random.randint(0, n2, batchsize)\n",
        "\n",
        "        x_data = (x1[num1] + x2[num2]) / 2.0\n",
        "        y_data = (x1[num1], x2[num2])\n",
        "\n",
        "        yield x_data, y_data"
      ],
      "metadata": {
        "id": "Msxd-j1j2g5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco una funzione per il controllo del bilanciamento delle classi.  \n",
        "Mi servirà per verificare che la divisione stratificata abbia avuto successo."
      ],
      "metadata": {
        "id": "0DKB7LG33wRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stampa_percentuale(y, title = \"\"):\n",
        "  if title:\n",
        "    print(title)\n",
        "  # Calcolo le occorrenze di ciascuna classe nel dataset.\n",
        "  unique, counts = np.unique(y, return_counts = True)\n",
        "  # Calcolo la % di occorrenze per ciascuna classe.\n",
        "  percentuali = (counts / len(y)) * 100\n",
        "  # Stampo le occorrenze e le percentuali.\n",
        "  for classe, conteggio, percentuale in zip(unique, counts, percentuali):\n",
        "      print(f\"Classe {classe}: Occorrenze = {conteggio}, Percentuale {percentuale} %\" )\n",
        "  print(f\"Totale occorrenze : {sum(counts)}\")\n",
        "  print()"
      ],
      "metadata": {
        "id": "3z92DZs935J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco un modello banale.\n",
        "Mi servirà per valutare che la rete abbia prestazioni migliori di esso."
      ],
      "metadata": {
        "id": "mdVdsGyY38FY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ide_model(x):\n",
        "   return((x,x))"
      ],
      "metadata": {
        "id": "Vyn_ESXT4F3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ora ha inizio la fase di caricamento e preparazione dei dataset che verranno utilizzati\n",
        "per addestrare e esaminare le prestazioni della rete.\n",
        "\n",
        "Prima di tutto, carico i training set e i testing set di \"MNIST\" e \"Fashion MNIST\"."
      ],
      "metadata": {
        "id": "mKhtLevU4QS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(mnist_x_train, mnist_y_train), (mnist_x_test, mnist_y_test) = mnist.load_data()\n",
        "(fashion_mnist_x_train, fashion_mnist_y_train), (fashion_mnist_x_test, fashion_mnist_y_test) = fashion_mnist.load_data()\n",
        "\n",
        "print(np.shape(mnist_x_train))"
      ],
      "metadata": {
        "id": "naZI3jI34Qgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ridimensiono le immagini tramite padding, portando la loro risoluzione da 28x28 a 32x32.  \n",
        "Inoltre, le normalizzo nell'intervallo [0, 1]."
      ],
      "metadata": {
        "id": "SgRovOk-4n4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#normalize in and pad\n",
        "mnist_x_train = np.pad(mnist_x_train,((0,0),(2,2),(2,2)))/255.\n",
        "mnist_x_test = np.pad(mnist_x_test,((0,0),(2,2),(2,2)))/255.\n",
        "fashion_mnist_x_train = np.pad(fashion_mnist_x_train,((0,0),(2,2),(2,2)))/255.\n",
        "fashion_mnist_x_test = np.pad(fashion_mnist_x_test,((0,0),(2,2),(2,2)))/255.\n",
        "\n",
        "print(np.shape(mnist_x_train))"
      ],
      "metadata": {
        "id": "6EkD9Wb246KM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Controllo il bilanciamento delle classi nei training set prima della divisione stratificata."
      ],
      "metadata": {
        "id": "dn2OwqKL7rbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stampa_percentuale(mnist_y_train, 'MNIST training set completo')\n",
        "stampa_percentuale(fashion_mnist_y_train, 'Fashion MNIST training set completo')"
      ],
      "metadata": {
        "id": "a7Xsfvub7uS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divido ogni training set in due insiemi: il training set e il validation set.\n",
        "Le immagini dei validation set verranno usate per l'ottimizzazione degli iper-parametri della rete.\n",
        "Ogni validation set conterrà il 20% delle immagini del training set di partenza.\n",
        "Uso la stratificazione per mantenere le classi nelle stesse proporzioni."
      ],
      "metadata": {
        "id": "b29NjrbE-V-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_x_train, mnist_x_val, mnist_y_train, mnist_y_val = sklearn.model_selection.train_test_split(mnist_x_train, mnist_y_train, test_size=0.2, stratify=mnist_y_train, random_state=seed)\n",
        "fashion_mnist_x_train, fashion_mnist_x_val, fashion_mnist_y_train, fashion_mnist_y_val = sklearn.model_selection.train_test_split(fashion_mnist_x_train, fashion_mnist_y_train, test_size=0.2, stratify=fashion_mnist_y_train, random_state=seed)"
      ],
      "metadata": {
        "id": "xENN8DNY-uZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Controllo il bilanciamento delle classi nei training set e nei validation set ottenuti dalla divisione stratificata."
      ],
      "metadata": {
        "id": "s8vTNUf2_VSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stampa_percentuale(mnist_y_train, 'MNIST training set risultante')\n",
        "stampa_percentuale(mnist_y_val, 'MNIST validation set risultante')\n",
        "\n",
        "stampa_percentuale(fashion_mnist_y_train, 'Fashion MNIST training set risultante')\n",
        "stampa_percentuale(fashion_mnist_y_val, 'Fashion MNIST validation set risultante')"
      ],
      "metadata": {
        "id": "Y88auHto_ccW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creo tre istanze del generatore per generare il training set, il validation set e il testing set finali."
      ],
      "metadata": {
        "id": "k4nmLpj8AjVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "traingen = datagenerator(mnist_x_train, fashion_mnist_x_train, training_set_size)\n",
        "valgen = datagenerator(mnist_x_val, fashion_mnist_x_val, validation_set_size)\n",
        "testgen = datagenerator(mnist_x_test, fashion_mnist_x_test, testing_set_size)"
      ],
      "metadata": {
        "id": "Isn3rYuF_reG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creo il training set e il validation set."
      ],
      "metadata": {
        "id": "LDBwhTE0BUC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = next(traingen)\n",
        "x_val, y_val = next(valgen)"
      ],
      "metadata": {
        "id": "TiPGk0YQBN9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifico che le dimensioni dei due dataset siano corrette."
      ],
      "metadata": {
        "id": "4onlfynIBmUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(x_val.shape)\n",
        "\n",
        "print(len(y_train))\n",
        "print(len(y_val))\n",
        "\n",
        "print(y_train[0].shape)\n",
        "print(y_train[1].shape)\n",
        "print(y_val[0].shape)\n",
        "print(y_val[1].shape)"
      ],
      "metadata": {
        "id": "7XXFelsHBZAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Controllo che i valori dei pixel delle immagini appartengano all'intervallo [0,1]."
      ],
      "metadata": {
        "id": "aXH31VvmB4Ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.min(x_train[0]), np.max(x_train[0]))\n",
        "print(np.min(x_val[0]), np.max(x_val[0]))"
      ],
      "metadata": {
        "id": "aJZI6GD3Bq_X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La fase di preparazione dei dataset è completa.\n",
        "Ora viene definita e compilata la rete neurale."
      ],
      "metadata": {
        "id": "EMfzMLnICB5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_neural_network():\n",
        "    # Input Layer\n",
        "    inputs = keras.Input(shape = image_shape, name = 'InputImage')\n",
        "\n",
        "    # Encoder 1\n",
        "    enc1_conv1 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', name = 'Enc1_Conv1')(inputs)\n",
        "    enc1_pool = keras.layers.MaxPooling2D((2, 2), padding='same', name = 'Enc1_Pool')(enc1_conv1)\n",
        "    enc1_conv2 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name = 'Enc1_Conv2')(enc1_pool)\n",
        "    encoded1 = keras.layers.MaxPooling2D((2, 2), padding='same', name = 'Encoded1')(enc1_conv2)\n",
        "\n",
        "    # Encoder 2\n",
        "    enc2_conv1 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', name = 'Enc2_Conv1')(inputs)\n",
        "    enc2_pool = keras.layers.MaxPooling2D((2, 2), padding='same', name = 'Enc2_Pool')(enc2_conv1)\n",
        "    enc2_conv2 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name = 'Enc2_Conv2')(enc2_pool)\n",
        "    encoded2 = keras.layers.MaxPooling2D((2, 2), padding='same', name = 'Encoded2')(enc2_conv2)\n",
        "\n",
        "    # Decoder 1\n",
        "    dec1_conv1 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name = 'Dec1_Conv1')(encoded1)\n",
        "    dec1_upsampling1 = keras.layers.UpSampling2D((2, 2), name = 'Dec1_Upsampling1')(dec1_conv1)\n",
        "    dec1_conv2 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', name = 'Dec1_Conv2')(dec1_upsampling1)\n",
        "    dec1_upsampling2 = keras.layers.UpSampling2D((2, 2), name = 'Dec1_Upsampling2')(dec1_conv2)\n",
        "    decoded1 = keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same', name = 'MNIST_Image')(dec1_upsampling2)\n",
        "\n",
        "    # Decoder 2\n",
        "    dec2_conv1 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same', name = 'Dec2_Conv1')(encoded2)\n",
        "    dec2_upsampling1 = keras.layers.UpSampling2D((2, 2), name = 'Dec2_Upsampling1')(dec2_conv1)\n",
        "    dec2_conv2 = keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same', name = 'Dec2_Conv2')(dec2_upsampling1)\n",
        "    dec2_upsampling2 = keras.layers.UpSampling2D((2, 2), name = 'Dec2_Upsampling2')(dec2_conv2)\n",
        "    decoded2 = keras.layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same', name = 'Fashion_MNIST_Image')(dec2_upsampling2)\n",
        "\n",
        "    model = keras.Model(inputs = inputs, outputs = [decoded1, decoded2], name = 'MNIST_Reconstruction_Model')\n",
        "    return model"
      ],
      "metadata": {
        "id": "25JonRhnB620"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Istanzio la rete e mostro i suoi dettagli."
      ],
      "metadata": {
        "id": "hHLaHd0nKYy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_neural_network()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "BcbYrLF7KXtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizzo un plot della rete."
      ],
      "metadata": {
        "id": "duLY3VwqMkv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keras.utils.plot_model(model, \"model.png\", show_shapes = True, show_layer_names = True)"
      ],
      "metadata": {
        "id": "1ATMl5ntKcye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco la funzione di costo che l'addestramento minimizzerà il più possibile."
      ],
      "metadata": {
        "id": "ZhSyZ298i5cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse_loss = keras.losses.MeanSquaredError(\n",
        "    reduction = \"sum_over_batch_size\",\n",
        "    name = \"mean_squared_error\",\n",
        "    dtype=None\n",
        ")"
      ],
      "metadata": {
        "id": "GZVnddnEjFAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco la metrica per valutare le prestazioni della rete sul validation set."
      ],
      "metadata": {
        "id": "c_1-xNi7jH4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mse_metric = keras.metrics.MeanSquaredError(name = \"mean_squared_error\", dtype = None)"
      ],
      "metadata": {
        "id": "6S9jz_FYjLfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilo la rete."
      ],
      "metadata": {
        "id": "R_PT0ZSwN9iF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss = [mse_loss, mse_loss],\n",
        "    optimizer = keras.optimizers.AdamW(learning_rate = learning_rate),\n",
        "    metrics = [mse_metric, mse_metric],\n",
        ")"
      ],
      "metadata": {
        "id": "XF25EscDMopF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco una callback EarlyStopping. Essa valuta alla fine di ogni epoca la loss sul validation set e decide se fermare l'addestramento oppure no. Utile per stabilire automaticamente un buon numero di epoche di addestramento della rete."
      ],
      "metadata": {
        "id": "YK3VpLnZOzQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor = 'val_loss',   # Monitora la loss sulla metrica indicata\n",
        "    min_delta = 0.001,      # Variazione minima da considerare come miglioramento\n",
        "    patience = early_stopping_patience,          # Numero di epoche senza miglioramenti prima di fermare l'addestramento\n",
        "    mode = 'auto',          # oppure prende \"min\", \"max\", seleziona la direzione in automatico\n",
        "    restore_best_weights = True,  # Ripristina i pesi migliori quando l'addestramento si ferma\n",
        "    start_from_epoch = 5    # Inizia il monitoraggio dall'epoca 5\n",
        ")"
      ],
      "metadata": {
        "id": "EKXKUyu3OtBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco una callback \"BackupAndRestore\".  \n",
        "Alla fine di ogni epoca di addestramento, essa salva la rete in un file di backup temporaneo.  \n",
        "Se il notebook dovesse bloccarsi a tempo di esecuzione, sarà possibile riavviare il training ripristinando l'ultimo stato salvato nel file di backup."
      ],
      "metadata": {
        "id": "Wq84_EZnk8w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "backup_and_restore = keras.callbacks.BackupAndRestore(backup_dir = \"/tmp/backup\")"
      ],
      "metadata": {
        "id": "d1kLCloDlQk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definisco una callback per il controllo del learning rate."
      ],
      "metadata": {
        "id": "fhs6nPEqlYRr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor = 'val_loss',\n",
        "    min_delta = 0.001,\n",
        "    factor = 0.2,\n",
        "    patience = reduce_lr_patience,\n",
        "    min_lr = 0.001\n",
        ")"
      ],
      "metadata": {
        "id": "JfaFCAsmlbAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Addestro la rete."
      ],
      "metadata": {
        "id": "RyIa-tbIPGlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, validation_data = (x_val, y_val), callbacks = [early_stopping, backup_and_restore, reduce_lr])"
      ],
      "metadata": {
        "id": "ab9vDfmTPJjD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}